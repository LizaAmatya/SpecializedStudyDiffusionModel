{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d3fb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from data import dataloader \n",
    "import torch.nn.functional as F\n",
    "from torch.amp import GradScaler\n",
    "from diffusion_utils import load_latest_checkpoint, save_checkpoint\n",
    "from diff_model import nn_model\n",
    "import gc\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d3b8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import MonitorParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b655db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d640b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "save_dir = \"weights/data_context/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982821a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 500\n",
    "n_feat = 64\n",
    "batch_size = 4\n",
    "in_channels = 3\n",
    "height = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9b651b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "beta1 = 1e-4\n",
    "beta2 = 0.02\n",
    "n_epoch = 32\n",
    "lrate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ac9ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct DDPM noise schedule\n",
    "b_t = (beta2 - beta1) * torch.linspace(0, 1, timesteps + 1, device=device) + beta1\n",
    "a_t = 1 - b_t\n",
    "ab_t = torch.cumsum(a_t.log(), dim=0).exp()\n",
    "ab_t[0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119b2b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f77c674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "nn_model.train()\n",
    "optim = torch.optim.Adam(nn_model.parameters(), lr=lrate, weight_decay=0.0001)\n",
    "# helper function: perturbs an image to a specified noise level\n",
    "def perturb_input(x, t, noise):\n",
    "    t = t.to(torch.long)\n",
    "    return (\n",
    "        ab_t.sqrt()[t, None, None, None] * x + (1 - ab_t[t, None, None, None]) * noise\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205e0531",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = GradScaler(\"cuda\")\n",
    "loss_file_path = os.path.join(save_dir, \"loss_val.csv\")\n",
    "grad_file = os.path.join(save_dir, 'mean_std.csv')\n",
    "all_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addc9c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(loss_file_path):\n",
    "    with open(loss_file_path, mode=\"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"epoch\", \"epoch_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a96565",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(grad_file):\n",
    "        grad_writer = csv.writer(grad_file)\n",
    "        grad_writer.writerow(['epoch', 'parameter', 'grad_mean', 'grad_std'])  # Header row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9b0dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "nn_model, optim, start_epoch, loss = load_latest_checkpoint(nn_model, optim, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ff1b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(nn_model, data_loader, start_epoch, n_epoch):\n",
    "    # Instantiate the monitor\n",
    "    monitor = MonitorParameters()\n",
    "    grad_stats = {}\n",
    "\n",
    "    # Register hooks to each layer of your model\n",
    "    for layer in nn_model.children():\n",
    "        layer.register_forward_hook(monitor)\n",
    "    \n",
    "    for ep in range(start_epoch, n_epoch):\n",
    "        print(f\"!!!epoch {ep}!!!\")\n",
    "\n",
    "        # linearly decay learning rate\n",
    "        optim.param_groups[0][\"lr\"] = lrate * (1 - ep / n_epoch)\n",
    "        epoch_loss = 0.0\n",
    "        accumulation_steps = 4\n",
    "\n",
    "        pbar = tqdm(data_loader, mininterval=2)\n",
    "        for i, (images, seg_masks, text_embeds) in enumerate(pbar):\n",
    "            # print('iamges dataloader', images.shape, seg_masks.shape, text_embeds.shape)\n",
    "            # Use images, segmentation masks, and text embeddings as inputs\n",
    "            optim.zero_grad()\n",
    "            images = images.to(device)\n",
    "            seg_masks = seg_masks.to(device)\n",
    "            text_embeds = text_embeds.to(device)\n",
    "\n",
    "            # perturb data\n",
    "            noise = torch.randn_like(images)\n",
    "            t = torch.randint(1, timesteps + 1, (images.shape[0],)).to(torch.float32).to(device)\n",
    "            # t_long = t.to(torch.long)\n",
    "            x_pert = perturb_input(images, t, noise)\n",
    "            \n",
    "            # with torch.autocast(device_type='cuda'):        #adding this memory better performance along with scaler as GradScaler\n",
    "            # with torch.no_grad():\n",
    "            # Forward pass\n",
    "            pred_noise = nn_model(images, t, text_embeds, seg_masks)\n",
    "            print('pred noise and noise', pred_noise.shape, noise.shape)\n",
    "            # loss is mean squared error between the predicted and true noise\n",
    "            loss = F.mse_loss(pred_noise, noise)\n",
    "            all_losses.append(loss.item())\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "            \n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                print(f'!!!!loss-- {loss}, is inf or nan')\n",
    "                continue    # skip iteration if loss invalid\n",
    "            \n",
    "            # scaler.scale(loss).backward()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Optionally, if you want to check gradients after a backward pass\n",
    "            # Ensure this is after your loss.backward() call\n",
    "            for name, param in nn_model.named_parameters():\n",
    "                if param.requires_grad and param.grad is not None:\n",
    "                    if name not in grad_stats: \n",
    "                        grad_stats[name] = {'mean': [], 'std': []}\n",
    "                    mean_grad = param.grad.data.mean().item()\n",
    "                    std_grad = param.grad.data.std().item()\n",
    "                    grad_stats[name]['mean'].append(mean_grad)\n",
    "                    grad_stats[name]['std'].append(std_grad)\n",
    "\n",
    "                    print(f\"{name} grad -- mean: {mean_grad:.4f}, std: {std_grad:.4f}\")\n",
    "                else:\n",
    "                    print(f'layer {name} grad is None', param.grad)\n",
    "\n",
    "            if (i + 1) % accumulation_steps == 0 or i == len(pbar):\n",
    "                # Clip gradients because gradients exploding that give Nan\n",
    "                # scaler.unscale_(optim)  # Unscale gradients of model parameters\n",
    "                torch.nn.utils.clip_grad_norm_(nn_model.parameters(), max_norm=1.0)  \n",
    "                optim.step()\n",
    "                # scaler.update()\n",
    "                optim.zero_grad()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        del images, seg_masks, text_embeds, loss, x_pert, noise, t\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        with open(loss_file_path, mode='a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([ep+1, epoch_loss/len(pbar)])\n",
    "            \n",
    "        # Save gradient mean and std to the file for each parameter for each epoch\n",
    "        with open(grad_file, mode='a', newline='') as f:\n",
    "            grad_writer = csv.writer(f)\n",
    "            for name, stats in grad_stats.items():\n",
    "                mean_grad = sum(stats['mean']) / len(stats['mean'])\n",
    "                std_grad = sum(stats['std']) / len(stats['std'])\n",
    "                grad_writer.writerow([ep, name, mean_grad, std_grad])\n",
    "\n",
    "            grad_stats.clear()  # Clear after each epoch\n",
    "\n",
    "        print(f\"Epoch [{ep + 1}/{n_epoch}], Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # save model periodically\n",
    "        if ep % 4 == 0 or ep == int(n_epoch - 1):\n",
    "            save_checkpoint(nn_model, optim, ep, epoch_loss, save_dir)\n",
    "            print(\"saved model at \" + save_dir + f\"model_{ep}.pth\")\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Saving all losses in another file\n",
    "    all_loss_file = os.path.join(save_dir, \"all_losses.csv\")\n",
    "    with open(all_loss_file, mode=\"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"loss\"])\n",
    "        for loss in all_losses:\n",
    "            writer.writerow([loss])\n",
    "            \n",
    "    # Plot losses\n",
    "\n",
    "    data = pd.read_csv(loss_file_path)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(data['epoch'], data['epoch_loss'], marker='o', linestyle='-',color='b', label=\"Training Loss\")\n",
    "    plt.title(\"Training Loss Over Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13fd772",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Test forward pass with real data from the data loader\n",
    "def test_model(model, data_loader):\n",
    "    nn_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, seg_masks, text_embeds in data_loader:\n",
    "            print('iamges dataloader', images.shape, seg_masks.shape, text_embeds.shape)\n",
    "            # Use images, segmentation masks, and text embeddings as inputs\n",
    "            t = torch.randn(\n",
    "                images.size(0), 1\n",
    "            )  # Random time steps for testing (replace as needed)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images, t, text_embeds, seg_masks)\n",
    "\n",
    "            print(\"Output shape:\", outputs.shape)\n",
    "            # Output should match the input image shape: (batch_size, in_channels, height, width)\n",
    "            break  # Test with one batch\n",
    "    # model.train()  # Set the model back to training mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fa1f24",
   "metadata": {},
   "source": [
    "Run the test\n",
    "test_model(nn_model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d8cf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_model(nn_model=nn_model, data_loader=dataloader, start_epoch=0, n_epoch=n_epoch)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
